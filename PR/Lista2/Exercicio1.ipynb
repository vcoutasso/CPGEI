{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista 2 - Exercício 1\n",
    "\n",
    "Gere dois conjuntos de dados, $X$ (treinamento) e $X_1$ (teste), com $N=1000$ exemplos constituídos por vetores tridimensionais que pertencem a três classes equiprováveis $\\omega_1$, $\\omega_2$ e $\\omega_3$. Os parâmetros das classes são:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "m_1 &= [0, 0, 0]^T \\\\\n",
    "m_2 &= [1, 2, 2]^T & S_1 = S_2 = S_3 = \\begin{bmatrix}0.8 & 0 & 0 \\\\ 0 & 0.8 & 0 \\\\ 0 & 0 & 0.8\\end{bmatrix} = \\sigma^2 I \\\\\n",
    "m_3 &= [3, 3, 4]^T\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Utilizando $X$, calcule o estimador ML da média e das matrizes de covariância para as três classes. Como assume-se que as três matrizes de covariância devem ser as mesmas, calcule para as três classes e realize uma média entre as três como a estimativa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from scipy.spatial import distance\n",
    "\n",
    "N = 1000\n",
    "\n",
    "mu  = np.array([[0, 0, 0], [1, 2, 2], [3, 3, 4]])\n",
    "cov = np.matrix([[0.8, 0, 0], [0, 0.8, 0], [0, 0, 0.8]])\n",
    "\n",
    "train = np.array([np.random.multivariate_normal(m, cov, N) for m in mu])\n",
    "test  = np.array([np.random.multivariate_normal(m, cov, N) for m in mu])\n",
    "\n",
    "nclass = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimativa das médias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04920336, -0.00533844,  0.04200233],\n",
       "       [ 0.9331735 ,  1.95149438,  2.01271409],\n",
       "       [ 2.98454873,  2.98521318,  4.03864776]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_ml = (1/N)*np.sum(train, axis=1)\n",
    "print(\"Estimativa das médias:\")\n",
    "mu_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimativa da matriz de covariância:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.77818638, 0.        , 0.        ],\n",
       "       [0.        , 0.81275615, 0.        ],\n",
       "       [0.        , 0.        , 0.8414038 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_ml = np.mean([(1/N)*np.sum((train[i] - mu_ml[i]) ** 2, axis=0) for i in range(nclass)], axis=1) * np.identity(nclass)\n",
    "print(\"Estimativa da matriz de covariância:\")\n",
    "cov_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Utilize o classificador com base em distância Euclideana para classificar os padrões de $X_1$, utilizando os valores computados (MLE) no item anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_euclidean = []\n",
    "for sample in np.vstack(test):\n",
    "    # Norma de ordem 2 é equivalente à distância euclidiana\n",
    "    distances = np.array([np.linalg.norm(sample - mu_ml[i], ord=2) for i in range(nclass)])\n",
    "    labels_euclidean.append(np.argmin(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Utilize o classificador com base em distância Mahalanobis para classificar os padrões de $X_1$, utilizando os valores computados (MLE) no item anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mahalanobis = []\n",
    "for sample in np.vstack(test):\n",
    "    distances = np.array([distance.mahalanobis(sample, mu_ml[i], np.linalg.inv(cov_ml)) for i in range(nclass)])\n",
    "    labels_mahalanobis.append(np.argmin(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Utilize o classificador de Bayes para classificar os padrões de $X_1$, utilizando os valores computados (MLE) no item anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_bayes = []\n",
    "for sample in np.vstack(test):\n",
    "    pred = 0\n",
    "    for i in range(1, nclass):\n",
    "        mean_diff = mu_ml[pred] - mu_ml[i]\n",
    "        if np.dot(mean_diff, sample) - 0.5 * (np.dot(mu_ml[pred], mu_ml[pred]) - np.dot(mu_ml[i], mu_ml[i])) < 0:\n",
    "            pred = i\n",
    "    labels_bayes.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Para cada caso, calcule o erro de classificação e compare os resultados. Por que os desempenhos são próximos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro de classificação\n",
      "\n",
      "Distância Euclideana:     5.90%\n",
      "Distância de Mahalanobis: 5.80%\n",
      "Classificador de Bayes:   5.90%\n"
     ]
    }
   ],
   "source": [
    "ground_truth = np.array([0] * N + [1] * N + [2] * N)\n",
    "total = ground_truth.shape[0]\n",
    "\n",
    "err_euclidean = np.count_nonzero(labels_euclidean != ground_truth) / total * 100\n",
    "err_mahalanobis = np.count_nonzero(labels_mahalanobis != ground_truth) / total * 100\n",
    "err_bayes = np.count_nonzero(labels_bayes != ground_truth) / total * 100\n",
    "\n",
    "print(\"Erro de classificação\\n\")\n",
    "print(\"Distância Euclideana:     {:.2f}%\".format(err_euclidean))\n",
    "print(\"Distância de Mahalanobis: {:.2f}%\".format(err_mahalanobis))\n",
    "print(\"Classificador de Bayes:   {:.2f}%\".format(err_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A acurácia dos classificadores baseados em distância são similares, pois a distância de Mahalanobis é equivalente à distância Euclideana para uma distribuição esférica, que é o caso dos dados gerados (a pequena diferença possivelmente se dá por conta do cálculo da inversa da matrix de covariância). Similarmente, o classificador de Bayes apresenta o mesmo desempenho, pois para distribuições esféricas (matriz de covariância diagonal), os classificadores baseados em distância são equivalentes ao de Bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitba0466bdbada4661b2f8d2a2d3e09ceb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
